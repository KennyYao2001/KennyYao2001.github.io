<!DOCTYPE html>
<html>


<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Keling Yao's Homepage</title>
  <link href="css/bootstrap.css" rel='stylesheet' type='text/css' />
  <!--<link rel="shortcut icon" href="../images/fav_icon.png" type="image/x-icon">-->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
  <!-- Bulma Version 0.7.5-->
  <link rel="stylesheet" href="https://unpkg.com/bulma@0.7.5/css/bulma.min.css" />
  <link href='css/style.css' rel='stylesheet' type='text/css'>
  <script defer src="font-awesome-5.9.0/js/brands.min.js"></script>
  <script defer src="font-awesome-5.9.0/js/fontawesome.min.js"></script>
  <script src="js/menuspy.js"></script>
  <style>
    body {
      font-size: 0.9em; /* Ë∞ÉÊï¥‰∏∫‰Ω†ÊÉ≥Ë¶ÅÁöÑÂ§ßÂ∞è */
    }
    .name-indent {
      margin-top: 0.5em; /* Adjust the value as needed for the desired indentation */
    }
  </style>
  
</head>

<body>
  <section class="section">
    <div class="container">
      <div class="columns">
        <div class="column is-2">
          <div class="sticky">
            <figure class="image is-128x128">
              <img class="is-rounded" src="./images/sfphoto.png">
            </figure>

            <div class="content">
              <h3 class="name-indent">Keling (Kenny) Yao</h3>
              <h3 class="name-indent">ÂßöÂèØÂ≤∫</h3>
              <h6>CMU | CUHKSZ | Berkeley</h6>
            </div>
            <!-- details -->
            <div class="details">
              <h3>EMAIL</h3>
              <p><a href="mailto:yaokeling527@gmail.com">yaokeling527<br>[at]gmail[dot]com</a></p>
            </div>
            <!-- social network icons -->
            <div class="social">
              <a href="https://github.com/KennyYao2001" target="_blank">
                <span class="fab fa-github fa-2x" style="display:inline; text-decoration: none"></span>
              </a>
              <a href="https://scholar.google.com/citations?user=d2kAaQgAAAAJ&hl=en" target="_blank">
                <span class="fab fa-google fa-2x" style="display:inline; text-decoration: none"></span>
              </a>
              <a href="https://www.linkedin.com/in/keling-yao-319581249/" target="_blank">
                <span class="fab fa-linkedin fa-2x" style="display:inline; text-decoration: none"></span>
              </a>
              <!--<a href="https://www.facebook.com/tianweishen.shen" target="_blank">-->
                <!--<span class="fab fa-facebook fa-2x" style="display:inline; text-decoration: none"></span>-->
              <!--</a>-->
              <!--<a href="https://twitter.com/HegongEngineer" target="_blank">-->
                <!--<span class="fab fa-twitter fa-2x" style="display:inline; text-decoration: none"></span>-->
              <!--</a>-->
            </div>


            <div id="sidebar" class="menu sticky is-hidden-mobile">
              <p class="menu-label"><b>Quick Links</b></p>
              <ul class="menu-list">
                <li><a href="#news">News</a></li>
                <li><a href="#intro">Intro</a></li>
                <li><a href="#research">Research</a></li>
              </ul>
            </div>
          </div>



        </div>
        <div class="column">
          <div class="content">
            <!--News-->
            <h3 id="news">
              News
            </h3>
            <ul>
              <li><span>[2025/05] üéâ I join Waymo as a perception engineer intern.</span></li>
              <li><span>[2025/04] üéâ One paper gets accepted to CVPR2025 workshop mobile AI.</span></li>
              <li><span>[2024/10] üéâ One paper gets accepted to WACV2025.</span></li>
              <li><span>[2024/08] üéâ I join CMU Robotics Institute as a graduate student.</span></li>
              <li><span>[2024/07] üéâ One paper gets accepted to ICML2024 workshop DMLR.</span></li>
              <li><span>[2022/08] üéâ I join UC Berkeley as a visiting student.</span></li>
              <li><span>[2020/09] üéâ I join CUHKSZ as a Data Science major undergraduate.</span></li>
            </ul>


            <!--Intro-->
            <h3 id="intro">Intro</h3>
            <p>Hi there <img src='https://github-production-user-asset-6210df.s3.amazonaws.com/24524555/238178097-766d336d-b87d-44ba-807c-c51de2bc6b4d.gif' alt='GIFÁ§∫‰æã' style='width: auto; height: 1em;'>, I am currently a graduate student majored in Master of Computer Vision at School of Computer Science, <strong>Carnegie Mellon University</strong>. Before that, I received my B.S. degree in Data Science (CS track) at <strong>the Chinese University of Hong Kong, Shenzhen</strong>. In 2022, I was a visiting student at the Department of EECS, <strong>University of California, Berkeley</strong>. <br/><br/>

              My professional and research interest lies in Computer Vision and Robotics Learning, aiming to enhance robots' 3D perception. I have the honor of being advised by <a href="https://vivecenter.berkeley.edu/people/allen-y-yang/" target="_blank" style="text-decoration: underline;">Prof. Allen Yang</a> from the OpenARK Lab at the Department of EECS, University of California, Berkeley. I was also honored to be advised by <a href="https://jianlong-fu.github.io/" target="_blank" style="text-decoration: underline;">Dr. Jianlong Fu</a> from the Multimedia Search and Mining Group, at Microsoft Research Asia, Beijing, China. My research areas include Vision-Language Model, 3D Computer Vision and VR/AR/MR. I would like to explore more interesting areas in 3D perception and Vision Foundation Model in the future!<br/><br/>

              <strong>You can find my <a href="./files/Kenny_Yao_Resume.pdf" target="_blank" style="text-decoration: underline;">CV</a> here</strong>!<br/><br/>
            </p>

            <!--Research-->
            <h3 id="research">Research</h3>
            <strong>Research Interest</strong>: 3D Computer Vision, Vision-Language Model, and VR/AR/MR<br/><br/>

            <h4>2024</h4>

            <!-- ... existing code ... -->
            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./images/research/MSRA.png" width="160">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <strong>Transferring Foundation Models for Generalizable Robotic Manipulation</strong><br>
                    Jiange Yang, Wenhui Tan, Chuhao Jin, <strong>Keling Yao</strong>, Bei Liu, Jianlong Fu, Ruihua Song, Gangshan Wu, Limin Wang<br>
                    <em>WACV</em>, 2025 <strong style="color: red;">(Oral)</strong><br>
                    <a href="https://arxiv.org/abs/2306.05716" target="_blank">[<u>arXiv</u>]</a>
                    <a href="https://www.youtube.com/watch?v=1m9wNzfp_4E&t=1s" target="_blank">[<u>Video</u>]</a>
                  </p>
                  <p>
                    We propose a novel paradigm that effectively leverages language-reasoning segmentation mask generated by internet-scale foundation models, to condition robot manipulation tasks.
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./images/research/dttd2.png" width="160">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <strong>Robust Digital-Twin Localization via An RGBD-based Transformer Network and A Comprehensive Evaluation on a Mobile Dataset</strong><br>
                    Zixun Huang*, <strong>Keling Yao*</strong>, Seth Z. Zhao*, Chuanyu Pan*, Tianjian Xu, Weiyu Feng, Allen Y. Yang<br>
                    <em>CVPR@MAI</em>, 2025<br>
                    <em>ICML@DMLR</em>, 2024<br>
                    <a href="https://github.com/augcog/DTTD2" target="_blank">[<u>project page</u>]</a>
                    <a href="https://arxiv.org/abs/2309.13570" target="_blank">[<u>arXiv</u>]</a>
                    <a href="https://youtu.be/QhYWyoPTmOk" target="_blank">[<u>Video</u>]</a>
                  </p>
                  <p>
                    We propose a transformer-based 6DoF pose estimator designed to achieve state-of-the-art accuracy under real-world noisy data. To systematically validate the new solution's performance against the prior art, we also introduce a novel RGBD dataset called Digital Twin Tracking Dataset v2 (DTTD2).
                  </p>
                </div>
                </div>
            </article>  




          </div>
        </div>
      </div>
    </div>
  </section>

  <footer style="text-align: left; margin-top: 2em; padding-left: 15em;">
    <p>Credit: web source from <a href="http://hansf.me/" target="_blank">Dr. Songfang Han</a></p>
  </footer>

  <script>
    var elm = document.querySelector('#sidebar');
    var ms = new MenuSpy(elm, {
      activeClass: 'is-active'
    });
  </script>


</body>

</html>